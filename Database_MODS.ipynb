{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a88038e",
   "metadata": {},
   "source": [
    "# MODS\n",
    "Molecular Oncology Database of sdAbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from abnumber import Chain\n",
    "import json\n",
    "import os, glob, re\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "# Columns of the final df\n",
    "columns = ['Source',\n",
    "           'Nanobody_ID',\n",
    "           'Species',\n",
    "           'Antigen',\n",
    "           'DOI',\n",
    "           'PBD',\n",
    "           'Sequence',\n",
    "           'CDR1',\n",
    "           'CDR2',\n",
    "           'CDR3',\n",
    "           'FR1',\n",
    "           'FR2',\n",
    "           'FR3',\n",
    "           'FR4',\n",
    "           'Antigen_type',\n",
    "           'Antigen_affinity_nM']\n",
    "\n",
    "\n",
    "project_path = '/Users/javi/Library/CloudStorage/GoogleDrive-javiersp4.mail@gmail.com/.shortcut-targets-by-id/1b8C40NjyKMxPdoX0dkX-gW70Vgt8wzHU/VHH_ANALYSIS'\n",
    "\n",
    "# from url\n",
    "\n",
    "# ican_path = os.path.join(project_path, 'Data/iCAN/iCAN_db.xls')\n",
    "ican_path = 'https://static-content.springer.com/esm/art%3A10.1186%2Fs12864-017-4204-6/MediaObjects/12864_2017_4204_MOESM4_ESM.xls'\n",
    "plabdab_path = 'https://opig.stats.ox.ac.uk/webapps/plabdab-nano/static/downloads/all_sequences.csv.gz'\n",
    "\n",
    "# downloaded\n",
    "\n",
    "# folder paths\n",
    "sabdab_path = os.path.join(project_path, 'Data/SAbDab-nano/all_nano_structures')\n",
    "nbthermo_path = os.path.join(project_path, 'Data/NbThermo/main.c8f2e3ee8beed8c7b505.js')\n",
    "sdAbDB_alpaca_path = os.path.join(project_path, 'Data/sdAb-DB/ALPACA')\n",
    "sdAbDB_llama_path = os.path.join(project_path, 'Data/sdAb-DB/LLAMA')\n",
    "sdAbDB_dromed_path = os.path.join(project_path, 'Data/sdAb-DB/DROMEDARIO')\n",
    "sdAbDB_bactrian_path = os.path.join(project_path, 'Data/sdAb-DB/BACTRIAN CAMEL')\n",
    "sdAbDB_unspecified_path = os.path.join(project_path,'Data/sdAb-DB/unspecified species')\n",
    "\n",
    "\n",
    "# file paths\n",
    "indi_abg_m_path = os.path.join(project_path, 'Data/INDI/abgenbank_meta.tsv')\n",
    "indi_abg_s_path = os.path.join(project_path, 'Data/INDI/abgenbank_sequence.tsv')\n",
    "indi_manual_m_path = os.path.join(project_path, 'Data/INDI/manual_meta.tsv')\n",
    "indi_manual_s_path = os.path.join(project_path, 'Data/INDI/manual_sequence.tsv')\n",
    "indi_structural_m_path = os.path.join(project_path, 'Data/INDI/structure_meta.tsv')\n",
    "indi_structural_s_path = os.path.join(project_path, 'Data/INDI/structure_sequence.tsv')\n",
    "\n",
    "\n",
    "###  Extract FR sequences ###\n",
    "\n",
    "def extract_fr_sequences(row):\n",
    "    sequence = row['Sequence']\n",
    "    cdr1 = row['CDR1']\n",
    "    cdr2 = row['CDR2']\n",
    "    cdr3 = row['CDR3']\n",
    "\n",
    "    fr1 = \"\"\n",
    "    fr2 = \"\"\n",
    "    fr3 = \"\"\n",
    "    fr4 = \"\"\n",
    "\n",
    "    # Find positions of CDRs\n",
    "    cdr1_start = sequence.find(cdr1)\n",
    "    cdr1_end = cdr1_start + len(cdr1) if cdr1_start != -1 else -1\n",
    "\n",
    "    cdr2_start = sequence.find(cdr2, cdr1_end if cdr1_end != -1 else 0)\n",
    "    cdr2_end = cdr2_start + len(cdr2) if cdr2_start != -1 else -1\n",
    "\n",
    "    cdr3_start = sequence.find(cdr3, cdr2_end if cdr2_end != -1 else 0)\n",
    "    cdr3_end = cdr3_start + len(cdr3) if cdr3_start != -1 else -1\n",
    "\n",
    "    # Extract FR sequences based on CDR positions\n",
    "    if cdr1_start != -1:\n",
    "        fr1 = sequence[:cdr1_start]\n",
    "        if cdr2_start != -1:\n",
    "            fr2 = sequence[cdr1_end:cdr2_start]\n",
    "            if cdr3_start != -1:\n",
    "                fr3 = sequence[cdr2_end:cdr3_start]\n",
    "                fr4 = sequence[cdr3_end:]\n",
    "            else:\n",
    "                fr3 = sequence[cdr2_end:]\n",
    "        else:\n",
    "            if cdr3_start != -1:\n",
    "                fr2 = sequence[cdr1_end:cdr3_start]\n",
    "                fr4 = sequence[cdr3_end:]\n",
    "            else:\n",
    "                fr2 = sequence[cdr1_end:]\n",
    "    else: # No CDR1 found, assume FR1 is empty\n",
    "         if cdr2_start != -1:\n",
    "            fr2 = sequence[:cdr2_start]\n",
    "            if cdr3_start != -1:\n",
    "                fr3 = sequence[cdr2_end:cdr3_start]\n",
    "                fr4 = sequence[cdr3_end:]\n",
    "            else:\n",
    "                fr3 = sequence[cdr2_end:]\n",
    "         else: # No CDR1 or CDR2 found\n",
    "            if cdr3_start != -1:\n",
    "                fr3 = sequence[:cdr3_start]\n",
    "                fr4 = sequence[cdr3_end:]\n",
    "            else: # No CDRs found\n",
    "                fr4 = sequence\n",
    "\n",
    "\n",
    "    return fr1, fr2, fr3, fr4\n",
    "\n",
    "### Splits sequences ###\n",
    "\n",
    "def _looks_like_heavy_chain(seq: str) -> bool:\n",
    "    \n",
    "    _AA = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    _VH_START = re.compile(r'^(QVQL|EVQL|QVEL|QVHL|QVVQ|QVQLV)', re.IGNORECASE)\n",
    "    \n",
    "    if not isinstance(seq, str): \n",
    "        return False\n",
    "    s = seq.strip().upper()\n",
    "    if not (80 <= len(s) <= 170):    \n",
    "        return False\n",
    "    if any(ch not in _AA for ch in s):\n",
    "        return False\n",
    "    return bool(_VH_START.match(s)) or True  \n",
    "\n",
    "def _split_one(seq: str):\n",
    "\n",
    "    cols = ['FR1','CDR1','FR2','CDR2','FR3','CDR3','FR4']\n",
    "    if not isinstance(seq, str) or not seq.strip():\n",
    "        return pd.Series([None]*7, index=cols)\n",
    "    try:\n",
    "        ch = Chain(seq.strip().upper(), scheme='imgt')\n",
    "        return pd.Series([\n",
    "            str(ch.fr1_seq) if ch.fr1_seq else None,\n",
    "            str(ch.cdr1_seq) if ch.cdr1_seq else None,\n",
    "            str(ch.fr2_seq) if ch.fr2_seq else None,\n",
    "            str(ch.cdr2_seq) if ch.cdr2_seq else None,\n",
    "            str(ch.fr3_seq) if ch.fr3_seq else None,\n",
    "            str(ch.cdr3_seq) if ch.cdr3_seq else None,\n",
    "            str(ch.fr4_seq) if ch.fr4_seq else None,\n",
    "        ], index=cols)\n",
    "    except Exception as e:\n",
    "        return pd.Series([None]*7, index=cols)\n",
    "\n",
    "def fill_fr_cdr_abnumber(df: pd.DataFrame, seq_col: str = 'Sequence', only_heavy_like: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    for c in ['FR1','FR2','FR3','FR4','CDR1','CDR2','CDR3']:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "\n",
    "    if only_heavy_like:\n",
    "        mask = out[seq_col].apply(_looks_like_heavy_chain)\n",
    "        splits = out.loc[mask, seq_col].apply(_split_one)\n",
    "        out.loc[mask, ['FR1','CDR1','FR2','CDR2','FR3','CDR3','FR4']] = splits.values\n",
    "    else:\n",
    "        splits = out[seq_col].apply(_split_one)\n",
    "        for c in ['FR1','CDR1','FR2','CDR2','FR3','CDR3','FR4']:\n",
    "            out[c] = splits[c]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869e20e",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51982b6",
   "metadata": {},
   "source": [
    "## PLabDab-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plabdab_raw = pd.read_csv(plabdab_path)\n",
    "\n",
    "print(plabdab_raw.info())\n",
    "plabdab_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86633d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plabdab = plabdab_raw.copy()\n",
    "plabdab.drop(columns=['source'], inplace=True)\n",
    "plabdab['Source'] = 'PLabDab-nano'\n",
    "\n",
    "# Extract CDR sequences\n",
    "plabdab['cdr_sequences'] = plabdab['cdr_sequences'].apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "plabdab['CDR1'] = plabdab['cdr_sequences'].apply(lambda x: x.get('CDRH1', ''))\n",
    "plabdab['CDR2'] = plabdab['cdr_sequences'].apply(lambda x: x.get('CDRH2', ''))\n",
    "plabdab['CDR3'] = plabdab['cdr_sequences'].apply(lambda x: x.get('CDRH3', ''))\n",
    "\n",
    "# Cleaning columns\n",
    "plabdab.rename(columns={'sequence': 'Sequence',\n",
    "                        'ID': 'Nanobody_ID',\n",
    "                        'organism': 'Species',\n",
    "                        'targets_mentioned': 'Antigen'}, inplace=True)\n",
    "\n",
    "plabdab[['FR1', 'FR2', 'FR3', 'FR4']] = plabdab.apply(extract_fr_sequences, axis=1, result_type='expand')\n",
    "\n",
    "\n",
    "plabdab['DOI']=None\n",
    "plabdab['PBD']=None\n",
    "plabdab['Antigen_type']=None\n",
    "plabdab['Antigen_affinity_nM']=None\n",
    "\n",
    "# Select and reorder columns\n",
    "plabdab = plabdab[columns]\n",
    "plabdab.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plabdab.info()\n",
    "plabdab.to_csv('plabdab_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a529f",
   "metadata": {},
   "source": [
    "## SAbDab-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KEYWORDS = (\n",
    "    \"NANOBODY\",\"VHH\",\"SINGLE-DOMAIN\",\"SINGLE DOMAIN\",\"CAMELID\",\n",
    "    \"HEAVY CHAIN ANTIBODY\",\"VH\",\"SINGLE DOMAIN ANTIBODY\"\n",
    ")\n",
    "\n",
    "def read_header(path):\n",
    "    parser = PDBParser(QUIET=True) \n",
    "    structure_id = os.path.splitext(os.path.basename(path))[0]\n",
    "    structure = parser.get_structure(structure_id, path)\n",
    "    header = parser.get_header() \n",
    "    doi = None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            text = fh.read()\n",
    "        m = re.search(r\"10\\.\\d{4,9}/\\S+\\w\", text)\n",
    "        if m:\n",
    "            doi = m.group(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return header, doi\n",
    "\n",
    "def seqres_sequences(path):\n",
    "\n",
    "    chain_to_seq = {}\n",
    "    for rec in SeqIO.parse(path, \"pdb-seqres\"):\n",
    "        chain = None\n",
    "        if \":\" in rec.id:\n",
    "            chain = rec.id.split(\":\")[-1]\n",
    "        elif \"_\" in rec.id:\n",
    "            chain = rec.id.split(\"_\")[-1]\n",
    "        if chain:\n",
    "            chain_to_seq[chain] = str(rec.seq).upper()\n",
    "    return chain_to_seq\n",
    "\n",
    "def normalize_compnd_source(header):\n",
    "\n",
    "    chain2mol, chain2org = {}, {}\n",
    "    comp = header.get(\"compound\") or {}\n",
    "    src  = header.get(\"source\")   or {}\n",
    "\n",
    "    mol_to_chains = defaultdict(list)\n",
    "    for mol_id, d in comp.items():\n",
    "        molname = (d.get(\"molecule\") or \"\").strip()\n",
    "        chains = [c.strip() for c in str(d.get(\"chain\") or \"\").split(\",\") if c.strip()]\n",
    "        for c in chains:\n",
    "            chain2mol[c] = molname\n",
    "            mol_to_chains[mol_id].append(c)\n",
    "\n",
    "    for mol_id, d in src.items():\n",
    "        org = (d.get(\"organism_scientific\") or d.get(\"organism_common\") or \"\").strip()\n",
    "        chains = mol_to_chains.get(mol_id, [])\n",
    "        for c in chains:\n",
    "            chain2org[c] = org\n",
    "\n",
    "    return chain2mol, chain2org\n",
    "\n",
    "def detect_nanobody_chains(chain2mol, chain2seq, title):\n",
    "    title_u = (title or \"\").upper()\n",
    "    nb = [c for c, mol in chain2mol.items() if any(k in (mol or \"\").upper() for k in NB_KEYWORDS)]\n",
    "    if nb:\n",
    "        return sorted(set(nb))\n",
    "    if any(k in title_u for k in (\"NANOBODY\",\"VHH\",\"ANTIBODY\")):\n",
    "        for c, seq in chain2seq.items():\n",
    "            if 95 <= len(seq) <= 140:\n",
    "                nb.append(c)\n",
    "    return sorted(set(nb))\n",
    "\n",
    "def infer_antigen(chain2mol, chain2seq, nb_chains):\n",
    "    non_nb = [c for c in chain2seq if c not in nb_chains]\n",
    "    if not non_nb:\n",
    "        return None, None\n",
    "    by_mol = defaultdict(list)\n",
    "    for c in non_nb:\n",
    "        molname = (chain2mol.get(c) or \"Unknown protein\").strip()\n",
    "        by_mol[molname].append(c)\n",
    "    def total_len(chs): return sum(len(chain2seq.get(ch,\"\")) for ch in chs)\n",
    "    antigen_mol, chains = max(by_mol.items(), key=lambda kv: total_len(kv[1]))\n",
    "    antigen_type = \"protein\" if total_len(chains) > 0 else None\n",
    "    return antigen_mol, antigen_type\n",
    "\n",
    "def process_pdb_simple(path):\n",
    "    header, doi = read_header(path)\n",
    "    pdb_id = os.path.splitext(os.path.basename(path))[0].upper()\n",
    "    title  = header.get(\"name\") or \"\"\n",
    "    chain2mol, chain2org = normalize_compnd_source(header)\n",
    "    chain2seq = seqres_sequences(path)\n",
    "\n",
    "    nb_chains = detect_nanobody_chains(chain2mol, chain2seq, title)\n",
    "    antigen_name, antigen_type = infer_antigen(chain2mol, chain2seq, nb_chains)\n",
    "\n",
    "    rows = []\n",
    "    for c in nb_chains:\n",
    "        rows.append({\n",
    "            'Source': 'SAbDab-nano',\n",
    "            'Nanobody_ID': f\"{pdb_id}_{c}\",\n",
    "            'Species': chain2org.get(c) or None,\n",
    "            'Antigen': antigen_name,\n",
    "            'DOI': doi,\n",
    "            'PBD': pdb_id,\n",
    "            'Sequence': chain2seq.get(c),\n",
    "            'CDR1': None, 'CDR2': None, 'CDR3': None,\n",
    "            'FR1': None, 'FR2': None, 'FR3': None, 'FR4': None,\n",
    "            'Antigen_type': antigen_type,\n",
    "            'Antigen_affinity_nM': None\n",
    "        })\n",
    "    return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Process folder\n",
    "\n",
    "all_rows = []\n",
    "pdb_files = sorted(glob.glob(os.path.join(sabdab_path, \"*.pdb\")))\n",
    "print(f\"Encontrados {len(pdb_files)} PDBs en {sabdab_path}\")\n",
    "\n",
    "for p in pdb_files:\n",
    "    try:\n",
    "        df_one = process_pdb_simple(p)\n",
    "        if not df_one.empty:\n",
    "            all_rows.append(df_one)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {os.path.basename(p)} -> {e}\")\n",
    "\n",
    "sabdab_df = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=columns)\n",
    "print(\"Nanobody-chains:\", len(sabdab_df))\n",
    "sabdab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb525bd",
   "metadata": {},
   "source": [
    "## ICAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3389d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ican_raw = pd.read_excel(ican_path)\n",
    "ican_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ddb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "\n",
    "ican = ican_raw.copy()\n",
    "ican.rename(columns={'Name': 'Nanobody_ID',\n",
    "                     'Antigen ': 'Antigen',\n",
    "                     'Antigen source': 'Antigen_type',\n",
    "                     'Source Organism': 'Species',\n",
    "                     'Protein seq': 'Sequence',\n",
    "                     'PDB ID': 'PBD',\n",
    "                     'CDR1 seq': 'CDR1',\n",
    "                     'CDR2 seq': 'CDR2',\n",
    "                     'CDR3 seq': 'CDR3',\n",
    "                     }, inplace=True)\n",
    "\n",
    "ican['DOI']=None\n",
    "ican['Antigen_affinity_nM']=None\n",
    "ican['FR1']=None\n",
    "ican['FR2']=None\n",
    "ican['FR3']=None\n",
    "ican['FR4']=None\n",
    "\n",
    "ican['Source'] = 'ICAN'\n",
    "\n",
    "ican[['FR1', 'FR2', 'FR3', 'FR4']] = ican.apply(extract_fr_sequences, axis=1, result_type='expand')\n",
    "\n",
    "ican = ican[columns]\n",
    "print(ican.info())\n",
    "ican.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560147dd",
   "metadata": {},
   "source": [
    "## INDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566bc80",
   "metadata": {},
   "source": [
    "### Abgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AbGenbank sequence\n",
    "abgen_s = pd.read_csv(indi_abg_s_path, sep='\\t')\n",
    "abgen_s.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AbGenbank metadata\n",
    "abgen_m = pd.read_csv(indi_abg_m_path, sep='\\t')\n",
    "abgen_m.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_antigen_from_description(text: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Try to extract an antigen token from a description string.\n",
    "    Heuristics:\n",
    "      1) 'anti-XXXX'  -> return XXXX\n",
    "      2) 'against XXXX' -> return XXXX\n",
    "    Allowed token chars: letters, digits, hyphen, underscore, dot.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    s = text.strip()\n",
    "\n",
    "    # Pattern 1: 'anti-XXXX'\n",
    "    m = re.search(r'anti-([A-Za-z0-9][A-Za-z0-9\\-\\._]*)', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        token = m.group(1).strip('.,;:()[]{} ')\n",
    "        return token or None\n",
    "\n",
    "    # Pattern 2: 'against XXXX'\n",
    "    m = re.search(r'\\bagainst\\s+([A-Za-z0-9][A-Za-z0-9\\-\\._]*)', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        token = m.group(1).strip('.,;:()[]{} ')\n",
    "        return token or None\n",
    "\n",
    "    return None\n",
    "\n",
    "def df_creation(sequence, metadata, antigen_type_rule: str = \"none\"):\n",
    "\n",
    "    # Validate required columns\n",
    "    required_meta_cols = {'id', 'organism', 'description', 'nucleotide sequence'}\n",
    "    required_seq_cols  = {'sequence', 'CDR1', 'CDR2', 'CDR3', 'Comma-separated db-specific IDS'}\n",
    "    missing_meta = required_meta_cols - set(metadata.columns)\n",
    "    missing_seq  = required_seq_cols  - set(sequence.columns)\n",
    "    if missing_meta:\n",
    "        raise ValueError(f\"Missing required metadata columns: {missing_meta}\")\n",
    "    if missing_seq:\n",
    "        raise ValueError(f\"Missing required sequence columns: {missing_seq}\")\n",
    "\n",
    "    # Normalize IDs in meta\n",
    "    metadata = metadata.copy()\n",
    "    metadata['id'] = metadata['id'].astype(str).str.strip()\n",
    "\n",
    "    # Normalize and explode accessions in the sequence table\n",
    "    sequence = sequence.copy()\n",
    "    sequence['Comma-separated db-specific IDS'] = sequence['Comma-separated db-specific IDS'].fillna('').astype(str)\n",
    "    sequence['accession_list'] = sequence['Comma-separated db-specific IDS'].apply(\n",
    "        lambda s: [tok.strip() for tok in s.split(',') if tok.strip()]\n",
    "    )\n",
    "    seqs_exploded = sequence.explode('accession_list', ignore_index=True).rename(\n",
    "        columns={'accession_list': 'accession'}\n",
    "    )\n",
    "\n",
    "    # Merge m:1 (many sequence rows -> one meta row)\n",
    "    merged = seqs_exploded.merge(\n",
    "        metadata,\n",
    "        left_on='accession',\n",
    "        right_on='id',\n",
    "        how='left',\n",
    "        validate='m:1'\n",
    "    )\n",
    "\n",
    "    # Map merged rows to the final schema\n",
    "    rows = []\n",
    "    for _, r in merged.iterrows():\n",
    "        nb_id = r.get('id') or r.get('accession') or None\n",
    "        species = r.get('organism') or None\n",
    "        antigen = _parse_antigen_from_description(r.get('description'))\n",
    "\n",
    "        doi = None\n",
    "        pbd = None\n",
    "        sequence = r.get('sequence') or None\n",
    "\n",
    "        cdr1 = r.get('CDR1') or None\n",
    "        cdr2 = r.get('CDR2') or None\n",
    "        cdr3 = r.get('CDR3') or None\n",
    "\n",
    "        fr1 = fr2 = fr3 = fr4 = None\n",
    "        if antigen_type_rule == \"protein_if_antigen\":\n",
    "            antigen_type = \"protein\" if antigen else None\n",
    "        else:\n",
    "            antigen_type = None\n",
    "\n",
    "        antigen_affinity = None\n",
    "\n",
    "        rows.append({\n",
    "            'Nanobody_ID': nb_id,\n",
    "            'Species': species,\n",
    "            'Antigen': antigen,\n",
    "            'DOI': doi,\n",
    "            'PBD': pbd,\n",
    "            'Sequence': sequence,\n",
    "            'CDR1': cdr1,\n",
    "            'CDR2': cdr2,\n",
    "            'CDR3': cdr3,\n",
    "            'FR1': fr1,\n",
    "            'FR2': fr2,\n",
    "            'FR3': fr3,\n",
    "            'FR4': fr4,\n",
    "            'Antigen_type': antigen_type,\n",
    "            'Antigen_affinity_nM': antigen_affinity\n",
    "        })\n",
    "\n",
    "    final_df = pd.DataFrame(rows, columns=columns)\n",
    "    final_df = final_df.drop_duplicates().reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "# df creation and formatting\n",
    "abgen_df = df_creation(abgen_s, abgen_m)\n",
    "\n",
    "abgen_df['Source'] = 'INDI - AbGenbank'\n",
    "abgen_df[['FR1', 'FR2', 'FR3', 'FR4']] = abgen_df.apply(extract_fr_sequences, axis=1, result_type='expand')\n",
    "\n",
    "abgen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823b43c",
   "metadata": {},
   "source": [
    "### Structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f02ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(indi_structural_m_path, sep=\"\\t\")\n",
    "seq  = pd.read_csv(indi_structural_s_path,  sep=\"\\t\")\n",
    "\n",
    "fixed_cols = {\"sequence\",\"CDR1\",\"CDR2\",\"CDR3\"}\n",
    "present_fixed = [c for c in fixed_cols if c in seq.columns]\n",
    "id_cols = [c for c in seq.columns if c not in present_fixed]\n",
    "\n",
    "seq_long = seq.melt(\n",
    "    id_vars=present_fixed,\n",
    "    value_vars=id_cols,\n",
    "    var_name=\"id_col\",\n",
    "    value_name=\"raw_id\"\n",
    ").dropna(subset=[\"raw_id\"])\n",
    "\n",
    "seq_long[\"raw_id\"] = seq_long[\"raw_id\"].astype(str)\n",
    "seq_long[\"raw_id\"] = seq_long[\"raw_id\"].str.split(r\"[,\\s;]+\")\n",
    "\n",
    "seq_long = seq_long.explode(\"raw_id\")\n",
    "\n",
    "seq_long[\"Nanobody_ID\"] = seq_long[\"raw_id\"].str.strip()\n",
    "\n",
    "# Merge\n",
    "meta_min = meta.rename(columns={\"id\": \"Nanobody_ID\"})\n",
    "merged = seq_long.merge(meta_min, on=\"Nanobody_ID\", how=\"left\")\n",
    "\n",
    "structural_df = pd.DataFrame({\n",
    "    \"Source\": \"INDI - Structural\",\n",
    "    \"Nanobody_ID\": merged[\"Nanobody_ID\"],\n",
    "    \"Species\": merged.get(\"species\"),\n",
    "    \"Antigen\": merged.get(\"antigen\"),\n",
    "    \"DOI\": merged.get(\"doi\"),\n",
    "    \"PBD\": merged.get(\"pdb\"),\n",
    "    \"Sequence\": merged.get(\"sequence\"),\n",
    "    \"CDR1\": merged.get(\"CDR1\"),\n",
    "    \"CDR2\": merged.get(\"CDR2\"),\n",
    "    \"CDR3\": merged.get(\"CDR3\"),\n",
    "    \"FR1\": None,\n",
    "    \"FR2\": None,\n",
    "    \"FR3\": None,\n",
    "    \"FR4\": None,\n",
    "    \"Antigen_type\": merged.get(\"antigen_type\"),\n",
    "    \"Antigen_affinity_nM\": merged.get(\"antigen_affinity_nM\"),\n",
    "})[columns]\n",
    "\n",
    "print(structural_df.shape)\n",
    "structural_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3fe077",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec89f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_manual_df(meta_path: str, seq_path: str) -> pd.DataFrame:\n",
    "    meta = pd.read_csv(meta_path, sep=\"\\t\") \n",
    "    seq  = pd.read_csv(seq_path,  sep=\"\\t\")  \n",
    "\n",
    "    ids_col = \"Comma-separated db-specific IDS\"\n",
    "    seq = seq.rename(columns={ids_col: \"ids\"})\n",
    "    seq[\"ids\"] = seq[\"ids\"].astype(str).fillna(\"\")\n",
    "    seq[\"ids\"] = seq[\"ids\"].str.split(\",\")\n",
    "    seq = seq.explode(\"ids\", ignore_index=True)\n",
    "    seq[\"ids\"] = seq[\"ids\"].astype(str).str.strip()\n",
    "\n",
    "    seq = seq[seq[\"ids\"] != \"\"].copy()\n",
    "\n",
    "    meta_min = meta.rename(columns={\"id\": \"Nanobody_ID\"})\n",
    "    seq_min  = seq.rename(columns={\n",
    "        \"ids\": \"Nanobody_ID\",\n",
    "        \"sequence\": \"Sequence\"\n",
    "    })\n",
    "    \n",
    "    meta_min = meta.rename(columns={\"id\": \"Nanobody_ID\"})[[\"Nanobody_ID\"]].drop_duplicates()\n",
    "\n",
    "    merged = pd.merge(seq_min, meta_min[[\"Nanobody_ID\"]], on=\"Nanobody_ID\", how=\"left\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Source\":              \"INDI - Manual\",\n",
    "        \"Nanobody_ID\":         merged[\"Nanobody_ID\"],\n",
    "        \"Species\":             None,\n",
    "        \"Antigen\":             None,\n",
    "        \"DOI\":                 None,\n",
    "        \"PBD\":                 None,\n",
    "        \"Sequence\":            merged[\"Sequence\"],\n",
    "        \"CDR1\":                merged.get(\"CDR1\"),\n",
    "        \"CDR2\":                merged.get(\"CDR2\"),\n",
    "        \"CDR3\":                merged.get(\"CDR3\"),\n",
    "        \"FR1\":                 None,\n",
    "        \"FR2\":                 None,\n",
    "        \"FR3\":                 None,\n",
    "        \"FR4\":                 None,\n",
    "        \"Antigen_type\":        None,\n",
    "        \"Antigen_affinity_nM\": None\n",
    "    })\n",
    "\n",
    "    out = out.reindex(columns=columns)\n",
    "    return out\n",
    "\n",
    "manual_df = build_manual_df(indi_manual_m_path, indi_manual_s_path)\n",
    "print(manual_df.shape)\n",
    "manual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74cc66b",
   "metadata": {},
   "source": [
    "## sdAb-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "_SEP = re.compile(r\"\\s*\\|\\s*\")\n",
    "\n",
    "def fasta_to_df(path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for rec in SeqIO.parse(path, \"fasta-pearson\"):\n",
    "        header = rec.description.strip()\n",
    "        parts = _SEP.split(header, maxsplit=2)\n",
    "        parts += [\"\"] * (3 - len(parts))\n",
    "        id_, species, antigen = parts[:3]\n",
    "        rows.append({\n",
    "            \"Nanobody_ID\": id_,\n",
    "            \"Species\": species.strip() or None,\n",
    "            \"Antigen\": antigen.strip() or None,\n",
    "            \"Sequence\": str(rec.seq).upper() or None\n",
    "        })\n",
    "    return pd.DataFrame(rows, columns=[\"Nanobody_ID\",\"Species\",\"Antigen\",\"Sequence\"])\n",
    "\n",
    "def sdab_folder_to_df(folder: str) -> pd.DataFrame:\n",
    "    files = sorted(Path(folder).glob(\"*.txt\"))\n",
    "    frames = [fasta_to_df(str(p)) for p in files]\n",
    "    return (pd.concat(frames, ignore_index=True)\n",
    "            if frames else pd.DataFrame(columns=[\"Nanobody_ID\",\"Species\",\"Antigen\",\"Sequence\"]))\n",
    "    \n",
    "\n",
    "sdab_llama = sdab_folder_to_df(sdAbDB_llama_path)\n",
    "sdab_alpaca = sdab_folder_to_df(sdAbDB_alpaca_path)\n",
    "sdab_arabcamel = sdab_folder_to_df(sdAbDB_dromed_path)\n",
    "sdab_bactriant = sdab_folder_to_df(sdAbDB_bactrian_path)\n",
    "sdab_unspecified = sdab_folder_to_df(sdAbDB_unspecified_path)\n",
    "\n",
    "# concat all the files\n",
    "sdab_all = pd.concat([sdab_llama, sdab_alpaca, sdab_arabcamel, sdab_bactriant, sdab_unspecified], ignore_index=True)\n",
    "\n",
    "sdab_all['Source'] = 'sdAb-DB'\n",
    "sdab_all['DOI'] = None\n",
    "sdab_all['PBD'] = None\n",
    "sdab_all['CDR1'] = None\n",
    "sdab_all['CDR2'] = None\n",
    "sdab_all['CDR3'] = None\n",
    "sdab_all['FR1'] = None\n",
    "sdab_all['FR2'] = None\n",
    "sdab_all['FR3'] = None\n",
    "sdab_all['FR4'] = None\n",
    "sdab_all['Antigen_type'] = None\n",
    "sdab_all['Antigen_affinity_nM'] = None\n",
    "\n",
    "sdab_all = sdab_all[columns]\n",
    "print(sdab_all.info())\n",
    "sdab_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfcc71",
   "metadata": {},
   "source": [
    "## NbThermo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _js_string_unescape(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Decode common JavaScript string escapes so that the content becomes valid JSON.\n",
    "    Handles sequences like \\n, \\t, \\\", \\\\, \\/, plus hex/unicode escapes.\n",
    "    \"\"\"\n",
    "    # Normalize escaped slashes used in JSON-in-JS\n",
    "    s = s.replace(r'\\/', '/')\n",
    "    # Decode using python's unicode_escape\n",
    "    return bytes(s, 'utf-8').decode('unicode_escape')\n",
    "\n",
    "def _find_json_string_in_js(js_text: str) -> str:\n",
    "    \"\"\"Locate the embedded JSON string inside main.js and return its raw content.\"\"\"\n",
    "    # Pattern A: JSON.parse(' ... ')\n",
    "    m = re.search(r'JSON\\.parse\\(\\s*([\\'\"])(.*?)\\1\\s*\\)', js_text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(2)\n",
    "    # Pattern B: var data='[...]';\n",
    "    m = re.search(r'\\b(?:var|let|const)\\s+\\w+\\s*=\\s*([\\'\"])(\\s*\\[\\s*\\{.*?\\}\\s*\\]\\s*)\\1', js_text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(2)\n",
    "    # Pattern C: fallback = longest quoted array\n",
    "    candidates = list(re.finditer(r'([\\'\"])(\\s*\\[\\s*\\{.*?\\}\\s*\\]\\s*)\\1', js_text, flags=re.DOTALL))\n",
    "    if candidates:\n",
    "        best = max(candidates, key=lambda mm: len(mm.group(2)))\n",
    "        return best.group(2)\n",
    "    raise ValueError(\"Could not locate embedded JSON string in main.js\")\n",
    "\n",
    "def _clean(s: Optional[str]) -> Optional[str]:\n",
    "    return s.strip() if isinstance(s, str) and s.strip() else None\n",
    "\n",
    "def _strip_hyphens(s: Optional[str]) -> Optional[str]:\n",
    "    return s.replace('-', '') if isinstance(s, str) else None\n",
    "\n",
    "def _pick_first_antigen(binding: Dict[str, Any]):\n",
    "    if not isinstance(binding, dict):\n",
    "        return (None, None, None)\n",
    "    for k in sorted([k for k in binding.keys() if k.lower().startswith(\"antigen\")]):\n",
    "        item = binding.get(k) or {}\n",
    "        name = _clean(item.get('name'))\n",
    "        typ  = _clean(item.get('type'))\n",
    "        aff  = item.get('affinity')\n",
    "        return (name, typ, aff if isinstance(aff, (int, float)) else None)\n",
    "    return (None, None, None)\n",
    "\n",
    "def _map_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    info    = rec.get('Info') or {}\n",
    "    origin  = rec.get('Origin') or {}\n",
    "    binding = rec.get('Binding') or {}\n",
    "    seq     = rec.get('Sequence') or {}\n",
    "    struct  = rec.get('Structure') or {}\n",
    "\n",
    "    doi = _clean((info.get('Reference') or {}).get('DOI'))\n",
    "    species = _clean((origin.get('Source') or {}).get('value'))\n",
    "    antigen_name, antigen_type, antigen_aff = _pick_first_antigen(binding)\n",
    "\n",
    "    seq_raw = _clean((seq.get('Raw') or {}).get('value'))\n",
    "    cdr1 = _strip_hyphens(_clean((seq.get('CDR1') or {}).get('value')))\n",
    "    cdr2 = _strip_hyphens(_clean((seq.get('CDR2') or {}).get('value')))\n",
    "    cdr3 = _strip_hyphens(_clean((seq.get('CDR3') or {}).get('value')))\n",
    "    fr1  = _strip_hyphens(_clean((seq.get('Framework1') or {}).get('value')))\n",
    "    fr2  = _strip_hyphens(_clean((seq.get('Framework2') or {}).get('value'))) or \\\n",
    "           _strip_hyphens(_clean((seq.get('Framework2') or {}).get('Values')))\n",
    "    fr3  = _strip_hyphens(_clean((seq.get('Framework3') or {}).get('value')))\n",
    "    fr4  = _strip_hyphens(_clean((seq.get('Framework4') or {}).get('value')))\n",
    "    pdb_val = _clean((struct.get('PDB') or {}).get('value'))\n",
    "\n",
    "    return {\n",
    "        'Nanobody_ID': _clean(rec.get('id')),\n",
    "        'Species': species,\n",
    "        'Antigen': antigen_name,\n",
    "        'DOI': doi,\n",
    "        'PBD': pdb_val,\n",
    "        'Sequence': seq_raw,\n",
    "        'CDR1': cdr1,\n",
    "        'CDR2': cdr2,\n",
    "        'CDR3': cdr3,\n",
    "        'FR1': fr1,\n",
    "        'FR2': fr2,\n",
    "        'FR3': fr3,\n",
    "        'FR4': fr4,\n",
    "        'Antigen_type': antigen_type,\n",
    "        'Antigen_affinity_nM': antigen_aff\n",
    "    }\n",
    "\n",
    "def nbthermo_from_main_js(js_path: str) -> pd.DataFrame:\n",
    "    with open(js_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        js_text = f.read()\n",
    "    raw_js_string = _find_json_string_in_js(js_text)\n",
    "    decoded = _js_string_unescape(raw_js_string)\n",
    "    data = json.loads(decoded)  # now it's valid JSON\n",
    "    rows = [_map_record(rec) for rec in data]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "filepath = os.path.join(project_path, 'Data/NbThermo/main.c8f2e3ee8beed8c7b505.js')\n",
    "nb_thermo = nbthermo_from_main_js(filepath)\n",
    "\n",
    "nb_thermo['Source'] = 'NbThermo'\n",
    "nb_thermo = nb_thermo[columns]\n",
    "nb_thermo.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d0ebe",
   "metadata": {},
   "source": [
    "# Merge dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dfs\n",
    "\n",
    "plabdab.to_csv('plabdab_df.csv')\n",
    "sabdab_df.to_csv('sabdab_df.csv')\n",
    "ican.to_csv('ican_df.csv')\n",
    "abgen_df.to_csv('INDI_abgen_df.csv')\n",
    "structural_df.to_csv('INDI_structural_df.csv')\n",
    "manual_df.to_csv('INDI_manual_df.csv')\n",
    "sdab_all.to_csv('sdAb-DB.csv')\n",
    "nb_thermo.to_csv('nb_thermo_df.csv')\n",
    "\n",
    "# load saved dfs\n",
    "\n",
    "saved_ican = pd.read_csv('ican_df.csv', index_col=0)\n",
    "saved_abgen = pd.read_csv('INDI_abgen_df.csv', index_col=0)\n",
    "saved_structural = pd.read_csv('INDI_structural_df.csv', index_col=0)\n",
    "saved_manual = pd.read_csv('INDI_manual_df.csv', index_col=0)\n",
    "saved_plabdab = pd.read_csv('plabdab_df.csv', index_col=0)\n",
    "saved_sabdab = pd.read_csv('sabdab_df.csv', index_col=0)\n",
    "saved_sdabdb = pd.read_csv('sdAb-DB_df.csv', index_col=0)\n",
    "saved_thermo = pd.read_csv('nb_thermo_df.csv', index_col=0)\n",
    "\n",
    "dfs = [saved_ican, saved_abgen, saved_structural, saved_manual, saved_plabdab, saved_sabdab, saved_sdabdb, saved_thermo]\n",
    "\n",
    "# merge dfs\n",
    "\n",
    "merged_df_databases = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# save\n",
    "merged_df_databases.to_csv('nanobodies_merged.csv')\n",
    "merged_df_databases.to_excel('nanobodies_merged.xlsx')\n",
    "\n",
    "print(merged_df_databases.info())\n",
    "merged_df_databases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898d9ef",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f911fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('nanobodies_merged.csv', index_col=0)\n",
    "print(merged_df.info())\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44e47c",
   "metadata": {},
   "source": [
    "## Remove empty species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['Species'].notnull()]\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a57f8f7",
   "metadata": {},
   "source": [
    "## Select species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Species'] = (\n",
    "    merged_df['Species']\n",
    "    .astype(str).str.strip().str.lower()\n",
    "    .replace({'nan': None, '': None})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b00be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_to_keep = ['camelus dromedarius', 'camelus bactrianus', \n",
    "       'lama glama', 'lama glama(lama pacos)', 'vicugna pacos', 'human cytomegalovirus, lama glama',\n",
    "       'lama pacos', 'llama glama', 'llama', 'lama', \"lama glama, escherichia coli (strain k12)\"\n",
    "       'lama glama', 'vicugna pacos huacaya', 'alpaca', 'arabian camel',\n",
    "       'bactrian camel']\n",
    "\n",
    "df_species_clean = merged_df[merged_df['Species'].isin(species_to_keep)]\n",
    "print(df_species_clean['Species'].unique())\n",
    "print(df_species_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9641811",
   "metadata": {},
   "source": [
    "## Consolidate categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_category_map = {\n",
    "    'vicugna pacos huacaya': 'Vicugna pacos',\n",
    "    'alpaca': 'Vicugna pacos',\n",
    "    'vicugna pacos': 'Vicugna pacos',\n",
    "    'lama glama': 'Lama glama',\n",
    "    'lama glama(lama pacos)': 'Lama glama',\n",
    "    'lama pacos': 'Lama glama',\n",
    "    'llama glama': 'Lama glama',\n",
    "    'llama': 'Lama glama',\n",
    "    'lama': 'Lama glama',\n",
    "    'lama glama, escherichia coli (strain k12)': 'Lama glama',\n",
    "    'human cytomegalovirus, lama glama': 'Lama glama',\n",
    "    'bactrian camel': 'Bactrian camel',\n",
    "    'camelus dromedarius': 'Camelus dromedarius',\n",
    "    'camelus bactrianus': 'Camelus dromedarius',\n",
    "    'arabian camel': 'Camelus dromedarius',\n",
    "}\n",
    "\n",
    "df_sp_con = df_species_clean.copy()\n",
    "df_sp_con['Species_Category'] = df_sp_con['Species'].map(species_category_map)\n",
    "\n",
    "df_sp_con.drop(columns=['Species'], inplace=True)\n",
    "df_sp_con.rename(columns={'Species_Category': 'Species'}, inplace=True)\n",
    "\n",
    "df_sp_con = df_sp_con[columns]\n",
    "print(df_sp_con.info())\n",
    "df_sp_con['Species'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20383e00",
   "metadata": {},
   "source": [
    "## Remove empty sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf45a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_sequences(df):\n",
    "    \n",
    "    # no sequence\n",
    "    noseq_ = len(df[df['Sequence'].isnull()])\n",
    "    # no sequence but with CDRs\n",
    "    nosequ_withcdr_ = len(df[(df['Sequence'].isnull()) & (df['CDR1'].notnull())])\n",
    "    # sequence and no CDRs\n",
    "    wseq_nocdr_ = len(df[(df['Sequence'].notnull()) & (df['CDR1'].isnull())])\n",
    "    # no sequence but with CDRs and FRs\n",
    "    noseq_wcdr_wfr_ = len(df[(df['Sequence'].isnull()) & (df['CDR1'].notnull()) & (df['FR1'].notnull())])\n",
    "    # sequence and CDRs and FRs\n",
    "    wseq_wcdr_wfr_ = len(df[(df['Sequence'].notnull()) & (df['CDR1'].notnull()) & (df['FR1'].notnull())])\n",
    "    # no sequence or no CDRs\n",
    "    noseq_or_nocdr_ = len(df[(df['Sequence'].isnull()) | (df['CDR1'].isnull())])\n",
    "    # no sequence and no CDRs\n",
    "    noseq_and_nocdr_ = len(df[(df['Sequence'].isnull()) & (df['CDR2'].isnull())])\n",
    "    print(f'no sequence: {noseq_}')\n",
    "    print(f'no sequence but with CDRs: {nosequ_withcdr_}')\n",
    "    print(f'sequence and no CDRs: {wseq_nocdr_}')\n",
    "    print(f'no sequence but with CDRs and FRs: {noseq_wcdr_wfr_}')\n",
    "    print(f'sequence and CDRs and FRs: {wseq_wcdr_wfr_}')\n",
    "    print(f'no sequence or no CDRs: {noseq_or_nocdr_}')\n",
    "    print(f'no sequence and no CDRs: {noseq_and_nocdr_}')\n",
    "    \n",
    "    return noseq_, nosequ_withcdr_, wseq_nocdr_, noseq_wcdr_wfr_, noseq_or_nocdr_, noseq_and_nocdr_ \n",
    "\n",
    "# Explore sequences\n",
    "noseq, nosequ_withcdr, wseq_nocdr, noseq_wcdr_wfr, noseq_or_nocdr, noseq_and_nocdr = explore_sequences(df_sp_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2432b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Empty Sequences and cdrs frs\n",
    "df_rm_no_seq_no_cdr = df_sp_con.dropna(subset=['Sequence', 'CDR1'], how='all')\n",
    "print(f'Before removal: {len(df_species_clean)}')\n",
    "print(f'After removal: {len(df_rm_no_seq_no_cdr)}')\n",
    "df_rm_no_seq_no_cdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d086a79",
   "metadata": {},
   "source": [
    "## Fill in empty CDRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa091b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill\n",
    "filled_cdrs = fill_fr_cdr_abnumber(df_rm_no_seq_no_cdr, seq_col='Sequence', only_heavy_like=True)\n",
    "wseq_nocdr_filled = len(filled_cdrs[(filled_cdrs['Sequence'].notnull()) & (filled_cdrs['CDR1'].isnull())])\n",
    "print(f'missing CDRs before: {wseq_nocdr}')\n",
    "print(f'missing CDRs after: {wseq_nocdr_filled}')\n",
    "\n",
    "# Check filled CDRs\n",
    "noseq_, nosequ_withcdr_, wseq_nocdr_, noseq_wcdr_wfr_, noseq_or_nocdr_, noseq_and_nocdr_ = explore_sequences(filled_cdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35516b0",
   "metadata": {},
   "source": [
    "## Reconstruct Sequence from CDRs and FRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_seq = filled_cdrs.copy()\n",
    "\n",
    "# Identify rows where Sequence is missing but CDRs and FRs are present\n",
    "condition = (recon_seq['Sequence'].isnull()) & \\\n",
    "            (recon_seq['CDR1'].notnull()) & (recon_seq['CDR2'].notnull()) & (recon_seq['CDR3'].notnull()) & \\\n",
    "            (recon_seq['FR1'].notnull()) & (recon_seq['FR2'].notnull()) & (recon_seq['FR3'].notnull()) & (recon_seq['FR4'].notnull())\n",
    "\n",
    "\n",
    "print(f'no seq but cdr and fr: {len(recon_seq[condition])}')\n",
    "\n",
    "# Reconstruct Sequence for these rows\n",
    "recon_seq.loc[condition, 'Sequence'] = recon_seq.loc[condition].apply(\n",
    "    lambda row: row['FR1'] + row['CDR1'] + row['FR2'] + row['CDR2'] + row['FR3'] + row['CDR3'] + row['FR4'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the reconstruction\n",
    "print(\"Number of rows with empty Sequence and non-empty CDRs/FRs before reconstruction:\")\n",
    "print(len(recon_seq[(recon_seq['Sequence'].isnull()) & (recon_seq['CDR1'].notnull()) & (recon_seq['FR1'].notnull())]))\n",
    "\n",
    "print(\"\\nNumber of rows with empty Sequence after reconstruction:\")\n",
    "print(len(recon_seq[recon_seq['Sequence'].isnull()]))\n",
    "noseq_, nosequ_withcdr_, wseq_nocdr_, noseq_wcdr_wfr_, noseq_or_nocdr_, noseq_and_nocdr_ = explore_sequences(recon_seq)\n",
    "print(recon_seq.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ea1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unsuccessful splits\n",
    "df_rm_no_cdr = recon_seq.dropna(subset=['CDR1'], how='all')\n",
    "print(f'Before removal: {len(recon_seq)}')\n",
    "print(f'After removal: {len(df_rm_no_cdr)}')\n",
    "df_rm_no_cdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5227a1",
   "metadata": {},
   "source": [
    "## Drop duplicates in Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and remove duplicates based on 'Sequence' column\n",
    "df_rm_duplicates = df_rm_no_cdr.copy()\n",
    "initial_rows = len(df_rm_duplicates)\n",
    "df_rm_duplicates.drop_duplicates(subset=['Sequence'], inplace=True)\n",
    "rows_after_dropping = len(df_rm_duplicates)\n",
    "duplicates_removed = initial_rows - rows_after_dropping\n",
    "\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "print(f\"Number of rows after dropping duplicates (based on Sequence): {rows_after_dropping}\")\n",
    "print(f\"Number of duplicate sequences removed: {duplicates_removed}\")\n",
    "\n",
    "df_rm_duplicates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f31a82",
   "metadata": {},
   "source": [
    "## Check for sorted CDRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d112f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_is_sorted(s: str) -> bool:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return False\n",
    "    return list(s) == sorted(s)\n",
    "\n",
    "df_rm_duplicates = df_rm_duplicates.copy()\n",
    "\n",
    "df_rm_duplicates['Any_CDR_sorted'] = df_rm_duplicates.apply(\n",
    "    lambda r: all(seq_is_sorted(r[c]) for c in ['CDR1','CDR2','CDR3']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter out the rows that meet the condition\n",
    "sorted_rows = df_rm_duplicates[df_rm_duplicates['Any_CDR_sorted']]\n",
    "\n",
    "print(\"Rows with any sorted CDR:\", len(sorted_rows))\n",
    "sorted_rows['Source'].value_counts()\n",
    "sorted_rows.head()\n",
    "df_rm_duplicates = df_rm_duplicates[~df_rm_duplicates['Any_CDR_sorted']]\n",
    "df_rm_duplicates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85d440",
   "metadata": {},
   "source": [
    "## Has not antigen or DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rm_duplicates = df_rm_duplicates.dropna(subset=['Antigen', 'DOI'], how='all')\n",
    "df_rm_duplicates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f785e",
   "metadata": {},
   "source": [
    "## Eliminate extra text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543edb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_predict_rows(df):\n",
    "    \"\"\"\n",
    "    Drop rows where any column contains the string '[predict]'.\n",
    "    \"\"\"\n",
    "    mask = df.applymap(lambda x: isinstance(x, str) and \"(predict)\" in x)\n",
    "    return df[~mask.any(axis=1)].copy()\n",
    "\n",
    "df_processed = drop_predict_rows(df_rm_duplicates)\n",
    "\n",
    "print(\"Before:\", len(df_rm_duplicates))\n",
    "print(\"After:\", len(df_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b2dff",
   "metadata": {},
   "source": [
    "## Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_excel('nanobodies_merged_clean.xlsx', index=False)\n",
    "df_processed.to_csv('nanobodies_merged_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded418b",
   "metadata": {},
   "source": [
    "# Graphs and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('nanobodies_merged_clean.csv')\n",
    "df_cleaned.head(2)\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2002d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts = df_cleaned['Species'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "species_counts.plot(kind='bar')\n",
    "plt.title('Number of nanobodies per Species', fontsize=22, pad=12)\n",
    "plt.xlabel('Species Category', fontsize=18, labelpad=8)\n",
    "plt.ylabel('Number of nanobodies', fontsize=18, labelpad=8)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dfdb4",
   "metadata": {},
   "source": [
    "### Descriptive statistics for the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_COLS = ['Sequence','CDR1','CDR2','CDR3','FR1','FR2','FR3','FR4']\n",
    "\n",
    "def length_describe(df: pd.DataFrame, by_species: bool = False):\n",
    "    \"\"\"\n",
    "    Compute descriptive stats of sequence lengths (Sequence, CDRs, FRs).\n",
    "    Does not add new columns to df.\n",
    "    \"\"\"\n",
    "    def _len(x):\n",
    "        return len(str(x)) if isinstance(x, str) else None\n",
    "\n",
    "    if by_species:\n",
    "        return (\n",
    "            df.groupby(\"Species\")[SEQ_COLS]\n",
    "              .agg(lambda col: col.dropna().map(_len).describe())\n",
    "        )\n",
    "    else:\n",
    "        return df[SEQ_COLS].agg(lambda col: col.dropna().map(_len).describe())\n",
    "\n",
    "global_stats = length_describe(df_cleaned, by_species=False)\n",
    "\n",
    "print(\"=== Global stats ===\")\n",
    "global_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _len_or_none(x):\n",
    "    return len(x) if isinstance(x, str) else None\n",
    "\n",
    "def length_stats_by_species(df: pd.DataFrame, cols=SEQ_COLS, combine=False):\n",
    "    \"\"\"\n",
    "    Compute per-Species descriptive stats of lengths for the given sequence columns.\n",
    "    Does NOT add any column to df.\n",
    "\n",
    "    Returns:\n",
    "      - if combine=False: dict {col_name: stats_df}\n",
    "      - if combine=True:  single DataFrame with a column MultiIndex (col_name, stat)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for col in cols:\n",
    "        tmp = df[['Species', col]].dropna(subset=[col]).copy()\n",
    "        tmp['len'] = tmp[col].map(_len_or_none)\n",
    "        # drop None lengths\n",
    "        tmp = tmp.dropna(subset=['len'])\n",
    "        # group and describe\n",
    "        stats = tmp.groupby('Species')['len'].describe()  # count, mean, std, min, 25%, 50%, 75%, max\n",
    "        results[col] = stats\n",
    "\n",
    "    if not combine:\n",
    "        return results\n",
    "\n",
    "    # Combine into a single DF with a MultiIndex on columns: (col_name, stat)\n",
    "    combined = pd.concat(results, axis=1)\n",
    "    # Optional: sort species and columns for readability\n",
    "    combined = combined.sort_index(axis=0).sort_index(axis=1, level=0)\n",
    "    return combined\n",
    "\n",
    "stats_dict = length_stats_by_species(df_cleaned, combine=True)\n",
    "stats_dict.head()\n",
    "stats = length_stats_by_species(df_cleaned, combine=False)\n",
    "stats['FR2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_by_species(df, col='CDR3'):\n",
    "    \"\"\"\n",
    "    Smoothed density curves (KDE) of sequence lengths by species.\n",
    "    \"\"\"\n",
    "    tmp = df[['Species', col]].dropna(subset=[col]).copy()\n",
    "    tmp['len'] = tmp[col].map(lambda x: len(x) if isinstance(x, str) else None)\n",
    "    tmp = tmp.dropna(subset=['len'])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    palette = plt.get_cmap(\"tab10\")\n",
    "\n",
    "    for i, sp in enumerate(tmp['Species'].unique()):\n",
    "        vals = tmp.loc[tmp['Species'] == sp, 'len']\n",
    "        sns.kdeplot(vals, label=str(sp), color=palette(i))\n",
    "\n",
    "    plt.xlabel(f\"{col} length (aa)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Smoothed distribution of {col} length by Species\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_kde_by_species(df_cleaned, col='FR2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75347c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_by_species(df, col='CDR3'):\n",
    "    \"\"\"\n",
    "    Boxplot of sequence lengths by species.\n",
    "    \"\"\"\n",
    "    tmp = df[['Species', col]].dropna(subset=[col]).copy()\n",
    "    tmp['len'] = tmp[col].map(lambda x: len(x) if isinstance(x, str) else None)\n",
    "    tmp = tmp.dropna(subset=['len'])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    palette = plt.get_cmap(\"tab10\")\n",
    "    species_list = tmp['Species'].unique()\n",
    "\n",
    "    data = [tmp.loc[tmp['Species'] == sp, 'len'].values for sp in species_list]\n",
    "    box = plt.boxplot(data,\n",
    "                      labels=[str(sp) for sp in species_list],\n",
    "                      patch_artist=True,\n",
    "                      showfliers=False)\n",
    "\n",
    "    for patch, color in zip(box['boxes'], [palette(i) for i in range(len(species_list))]):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    plt.ylabel(f\"{col} length (aa)\")\n",
    "    plt.title(f\"Boxplot of {col} length by Species\")\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_box_by_species(df_cleaned, col='CDR3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861aaa1",
   "metadata": {},
   "source": [
    "### Create expanded dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c30e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_length_columns(df, seq_cols=SEQ_COLS):\n",
    "    df = df.copy()\n",
    "    for col in seq_cols:\n",
    "        df[f\"{col}_len\"] = df[col].apply(lambda x: len(x) if isinstance(x, str) else None)\n",
    "    return df\n",
    "\n",
    "df_with_len = add_length_columns(df_cleaned)\n",
    "# df_with_len.to_csv('df_with_len.csv', index=False)\n",
    "df_with_len.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ace3a",
   "metadata": {},
   "source": [
    "### Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43328fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_multi(df, k=4):\n",
    "\n",
    "    df = df.copy()\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    len_cols = [c for c in df.columns if c.endswith(\"_len\")]\n",
    "\n",
    "    for col in len_cols:\n",
    "        series = df[col].dropna()\n",
    "        if series.empty:\n",
    "            continue\n",
    "        q1, q3 = series.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower, upper = q1 - k * iqr, q3 + k * iqr\n",
    "        mask &= df[col].between(lower, upper) | df[col].isna()\n",
    "\n",
    "    return df[mask]\n",
    "\n",
    "df_no_outliers = remove_outliers_multi(df_with_len)\n",
    "\n",
    "print(\"Before:\", len(df_with_len))\n",
    "print(\"After:\", len(df_no_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcead10",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_COLS = ['Sequence_len','CDR1_len','CDR2_len','CDR3_len','FR1_len','FR2_len','FR3_len','FR4_len']\n",
    "for i in SEQ_COLS:\n",
    "    plt.hist(df_with_len[i],bins=50)\n",
    "    plt.title({str(i)})\n",
    "    plt.show()\n",
    "df_no_outliers.to_csv('df_no_outliers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835caeb6",
   "metadata": {},
   "source": [
    "### Conservacin de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_VALID = set(list(\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "\n",
    "def _clean_seq(s: Optional[str]) -> Optional[str]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return None\n",
    "    s = s.strip().upper().replace(\" \", \"\")\n",
    "    s = \"\".join(ch if ch in AA_VALID else \"-\" for ch in s)\n",
    "    return s if s else None\n",
    "\n",
    "def _align_by_padding(seqs: List[str]) -> np.ndarray:\n",
    "    \"\"\"Pad right with '-' to the max length and return (N, L) char array.\"\"\"\n",
    "    if not seqs:\n",
    "        return np.empty((0, 0), dtype=\"<U1\")\n",
    "    L = max(len(s) for s in seqs)\n",
    "    arr = np.full((len(seqs), L), \"-\", dtype=\"<U1\")\n",
    "    for i, s in enumerate(seqs):\n",
    "        arr[i, :len(s)] = list(s)\n",
    "    return arr\n",
    "\n",
    "def _column_stats(col: np.ndarray) -> Dict[str, object]:\n",
    "    \"\"\"Stats for a single alignment column (array of single-char strings).\"\"\"\n",
    "    vals = [a for a in col if a != \"-\"]\n",
    "    n = len(vals)\n",
    "    if n == 0:\n",
    "        return {\"consensus\": \"-\", \"cons_freq\": 0.0, \"entropy\": 0.0, \"counts\": {}}\n",
    "    counts = Counter(vals)\n",
    "    aa_cons, cmax = max(counts.items(), key=lambda kv: kv[1])\n",
    "    pmax = cmax / n\n",
    "    # Shannon entropy (bits) over non-gap residues\n",
    "    H = -sum((c/n) * math.log2(c/n) for c in counts.values())\n",
    "    return {\"consensus\": aa_cons, \"cons_freq\": pmax, \"entropy\": H, \"counts\": dict(counts), \"n_eff\": n}\n",
    "\n",
    "def conservation_for_region(df: pd.DataFrame, fr_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute per-position conservation for one FR column (e.g., 'FR1').\n",
    "    Returns a DataFrame with:\n",
    "      pos, consensus, cons_freq (0-1), entropy (bits), n_eff, and counts (dict)\n",
    "    \"\"\"\n",
    "    seqs = [ _clean_seq(s) for s in df[fr_col].dropna().tolist() ]\n",
    "    seqs = [s for s in seqs if s and set(s) - {\"-\"}]  # keep non-empty\n",
    "    if not seqs:\n",
    "        return pd.DataFrame(columns=[\"pos\",\"consensus\",\"cons_freq\",\"entropy\",\"n_eff\",\"counts\"])\n",
    "    aln = _align_by_padding(seqs)  # (N, L)\n",
    "    stats = [ _column_stats(aln[:, j]) for j in range(aln.shape[1]) ]\n",
    "    out = pd.DataFrame(stats)\n",
    "    out.insert(0, \"pos\", np.arange(1, len(out)+1))  # 1-based\n",
    "    return out\n",
    "\n",
    "def conservation_all_FRs(df: pd.DataFrame, fr_cols: List[str] = [\"FR1\",\"FR2\",\"FR3\",\"FR4\"]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Compute conservation for each FR; returns dict {FR: stats_df}.\"\"\"\n",
    "    return { col: conservation_for_region(df, col) for col in fr_cols }\n",
    "\n",
    "def conservation_by_species(df: pd.DataFrame,\n",
    "                            species: str,\n",
    "                            fr_cols: List[str] = [\"FR1\",\"FR2\",\"FR3\",\"FR4\"]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Same as above, but restricted to a single Species value.\"\"\"\n",
    "    sub = df[df[\"Species\"] == species]\n",
    "    return conservation_all_FRs(sub, fr_cols=fr_cols)\n",
    "\n",
    "def consensus_strings(stats_dict: Dict[str, pd.DataFrame]) -> Dict[str, str]:\n",
    "    \"\"\"Build consensus sequence string per FR from stats tables.\"\"\"\n",
    "    cons = {}\n",
    "    for fr, s in stats_dict.items():\n",
    "        if s.empty:\n",
    "            cons[fr] = None\n",
    "        else:\n",
    "            cons[fr] = \"\".join(s[\"consensus\"].astype(str).tolist())\n",
    "    return cons\n",
    "\n",
    "fr_stats = conservation_all_FRs(df_no_outliers)\n",
    "\n",
    "fr3 = fr_stats[\"FR3\"]\n",
    "print(fr3.sort_values([\"cons_freq\", \"entropy\"], ascending=[False, True]).head(10))\n",
    "\n",
    "print(consensus_strings(fr_stats))\n",
    "llama_stats = conservation_by_species(df_no_outliers, species=\"Lama glama\")\n",
    "print(consensus_strings(llama_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conservation_dict(conservation_dict: dict, species_name: str) -> pd.DataFrame:\n",
    "\n",
    "    df = (\n",
    "        pd.concat(conservation_dict, names=[\"Region\"]) \n",
    "          .reset_index(level=0)                       \n",
    "          .assign(Species=species_name)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "species_list = [\"Lama glama\", \"Vicugna pacos\", \"Bactrian camel\", \"Camelus dromedarius\"]\n",
    "\n",
    "all_species_dfs = []\n",
    "for sp in species_list:\n",
    "    stats_dict = conservation_by_species(df_no_outliers, species=sp)\n",
    "    all_species_dfs.append(flatten_conservation_dict(stats_dict, sp))\n",
    "\n",
    "conservation_all = pd.concat(all_species_dfs, ignore_index=True)\n",
    "conservation_all.to_csv('conservation_by_species_.csv')\n",
    "conservation_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34072606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fr_conservation_heatmap(stats_dict, title=\"FR conservation heatmap\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of consensus frequency across all FRs.\n",
    "    Rows = FR regions (FR1..FR4), columns = positions (aligned per FR).\n",
    "    \"\"\"\n",
    "    frs = [\"FR1\",\"FR2\",\"FR3\",\"FR4\"]\n",
    "    data = []\n",
    "    lengths = []\n",
    "    for fr in frs:\n",
    "        if fr in stats_dict and not stats_dict[fr].empty:\n",
    "            vals = stats_dict[fr][\"entropy\"].values #cons_freq or entropy\n",
    "            data.append(vals)\n",
    "            lengths.append(len(vals))\n",
    "        else:\n",
    "            data.append([])\n",
    "            lengths.append(0)\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # pad with NaN so all rows have same length cons_freq\n",
    "    mat = np.full((len(frs), max_len), np.nan)\n",
    "    for i, vals in enumerate(data):\n",
    "        if len(vals) > 0:\n",
    "            mat[i, :len(vals)] = vals\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    im = plt.imshow(mat, aspect=\"auto\", cmap=\"plasma\", vmin=0, vmax=1)\n",
    "    plt.colorbar(im, label=\"Entropy\")\n",
    "    plt.yticks(range(len(frs)), frs)\n",
    "    plt.xlabel(\"Position (aligned per FR)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "fr_stats = conservation_all_FRs(df_no_outliers)\n",
    "plot_fr_conservation_heatmap(fr_stats, \"Framework conservation across all species - Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fr_conservation_by_species(df, fr_cols=[\"FR1\",\"FR2\",\"FR3\",\"FR4\"]):\n",
    "    \"\"\"\n",
    "    For each species in the DataFrame, compute FR conservation and plot a heatmap.\n",
    "    \"\"\"\n",
    "    species_list = sorted(df[\"Species\"].dropna().unique())\n",
    "\n",
    "    for sp in species_list:\n",
    "        sub_stats = conservation_by_species(df, species=sp, fr_cols=fr_cols)\n",
    "        plot_fr_conservation_heatmap(sub_stats, title=f\"FR conservation - Entropy  {sp}\")\n",
    "        \n",
    "plot_fr_conservation_by_species(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479382f",
   "metadata": {},
   "source": [
    "### Aminoacid stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9950889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amino_acid_stats(df, seq_cols=[\"Sequence\"], aa_list=None):\n",
    "    \"\"\"\n",
    "    Compute amino acid statistics for the given sequence columns in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with sequence columns (strings of amino acids).\n",
    "    seq_cols : list of str\n",
    "        Columns to analyze.\n",
    "    aa_list : list of str or None\n",
    "        If provided, restrict analysis to this set of amino acids.\n",
    "        If None, will use all 20 standard amino acids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats_dict : dict\n",
    "        {col_name: DataFrame with per-amino-acid counts, frequencies, mean per sequence, etc.}\n",
    "    \"\"\"\n",
    "    if aa_list is None:\n",
    "        aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")  # 20 standard amino acids\n",
    "\n",
    "    stats_dict = {}\n",
    "    for col in seq_cols:\n",
    "        seqs = df[col].dropna().astype(str)\n",
    "\n",
    "        # Global counts across all sequences\n",
    "        total_counts = Counter()\n",
    "        for s in seqs:\n",
    "            total_counts.update(s)\n",
    "\n",
    "        # Normalize to provided aa_list\n",
    "        total_counts = {aa: total_counts.get(aa, 0) for aa in aa_list}\n",
    "        total_residues = sum(total_counts.values())\n",
    "\n",
    "        # Per-sequence frequencies (useful for averages/variability)\n",
    "        per_seq = []\n",
    "        for s in seqs:\n",
    "            c = Counter(s)\n",
    "            per_seq.append([c.get(aa, 0) for aa in aa_list])\n",
    "        per_seq = pd.DataFrame(per_seq, columns=aa_list)\n",
    "\n",
    "        # Build stats table\n",
    "        stats = pd.DataFrame({\n",
    "            \"count_total\": total_counts,\n",
    "            \"freq_global\": {aa: total_counts[aa] / total_residues if total_residues else 0 for aa in aa_list},\n",
    "            \"mean_per_seq\": per_seq.mean().to_dict(),\n",
    "            \"std_per_seq\": per_seq.std().to_dict(),\n",
    "            \"min_per_seq\": per_seq.min().to_dict(),\n",
    "            \"max_per_seq\": per_seq.max().to_dict()\n",
    "        })\n",
    "        stats_dict[col] = stats\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "aa_stats = amino_acid_stats(df_no_outliers, seq_cols=[\"Sequence\"])\n",
    "\n",
    "print(aa_stats[\"Sequence\"].loc[[\"K\",\"C\"], [\"count_total\",\"freq_global\",\"mean_per_seq\"]])\n",
    "aa_table = aa_stats[\"Sequence\"].sort_values(\"freq_global\", ascending=False)\n",
    "pd.set_option(\"display.max_rows\", None)  \n",
    "display(aa_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aa_composition(stats, title=\"Amino acid composition in sequences\"):\n",
    "    stats = stats.sort_values(\"freq_global\", ascending=False)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(stats.index, stats[\"freq_global\"], color=\"teal\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Amino acid\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_aa_composition(aa_stats[\"Sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef97d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_composition_by_species(df, aa_list=list(\"ACDEFGHIKLMNPQRSTVWY\")):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame: rows=Species, cols=AA, values=freq_global.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sp, sub in df.groupby(\"Species\"):\n",
    "        seqs = sub[\"Sequence\"].dropna().astype(str)\n",
    "        total_counts = Counter()\n",
    "        for s in seqs:\n",
    "            total_counts.update(s)\n",
    "        total_counts = {aa: total_counts.get(aa, 0) for aa in aa_list}\n",
    "        total_residues = sum(total_counts.values())\n",
    "        freqs = {aa: (total_counts[aa]/total_residues if total_residues else 0) for aa in aa_list}\n",
    "        freqs[\"Species\"] = sp\n",
    "        results.append(freqs)\n",
    "    return pd.DataFrame(results).set_index(\"Species\")\n",
    "\n",
    "aa_species = aa_composition_by_species(df_no_outliers)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.heatmap(aa_species, cmap=\"plasma\", annot=False)\n",
    "plt.title(\"Amino acid composition per species\")\n",
    "plt.xlabel(\"Amino acid\")\n",
    "plt.ylabel(\"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_composition_by_species(df, seq_col=\"Sequence\", aa_list=list(\"ACDEFGHIKLMNPQRSTVWY\")):\n",
    "    \"\"\"\n",
    "    Compute amino acid frequencies per species.\n",
    "    Returns a dict {species: DataFrame of amino acid stats}.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for sp, sub in df.groupby(\"Species\"):\n",
    "        seqs = sub[seq_col].dropna().astype(str)\n",
    "\n",
    "        total_counts = Counter()        \n",
    "        for s in seqs:\n",
    "            total_counts.update(s)\n",
    "\n",
    "        # Fill missing amino acids with 0\n",
    "        total_counts = {aa: total_counts.get(aa, 0) for aa in aa_list}\n",
    "        total_residues = sum(total_counts.values())\n",
    "\n",
    "        # Per-sequence table\n",
    "        per_seq = []\n",
    "        for s in seqs:\n",
    "            c = Counter(s)\n",
    "            per_seq.append([c.get(aa, 0) for aa in aa_list])\n",
    "        per_seq = pd.DataFrame(per_seq, columns=aa_list)\n",
    "\n",
    "        stats = pd.DataFrame({\n",
    "            \"count_total\": total_counts,\n",
    "            \"freq_global\": {aa: total_counts[aa] / total_residues if total_residues else 0 for aa in aa_list},\n",
    "            \"mean_per_seq\": per_seq.mean().to_dict(),\n",
    "            \"std_per_seq\": per_seq.std().to_dict(),\n",
    "            \"min_per_seq\": per_seq.min().to_dict(),\n",
    "            \"max_per_seq\": per_seq.max().to_dict()\n",
    "        })\n",
    "\n",
    "        results[sp] = stats\n",
    "    return results\n",
    "\n",
    "aa_species_stats = aa_composition_by_species(df_no_outliers)\n",
    "aa_species_stats[\"Bactrian camel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aa_composition_species(stats_dict, species, title=None):\n",
    "    \"\"\"\n",
    "    Plot amino acid frequencies for a given species.\n",
    "    \"\"\"\n",
    "    stats = stats_dict[species].sort_values(\"freq_global\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(stats.index, stats[\"freq_global\"], color=\"seagreen\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Amino acid\")\n",
    "    if title is None:\n",
    "        title = f\"Amino acid composition  {species}\"\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_aa_composition_species(aa_species_stats, \"Lama glama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01585f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyses on K and C\n",
    "\n",
    "REGIONS = ['CDR1','CDR2','CDR3','FR1','FR2','FR3','FR4']\n",
    "AAS = ['K','C']  # Lysine & Cysteine\n",
    "\n",
    "def _counts_per_seq(seq: str, aas=AAS):\n",
    "    if not isinstance(seq, str):\n",
    "        return {aa: 0 for aa in aas}\n",
    "    c = Counter(seq)\n",
    "    return {aa: c.get(aa, 0) for aa in aas}\n",
    "\n",
    "def kc_region_tables(df: pd.DataFrame, regions=REGIONS, aas=AAS):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with per-region stats for K and C:\n",
    "      - *_count_total  (suma absoluta en la regin)\n",
    "      - *_freq_global  (proporcin sobre todos los aa observados en la regin)\n",
    "      - *_mean_per_seq (media por secuencia)\n",
    "      - *_std_per_seq  (desviacin por secuencia)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for col in regions:\n",
    "        seqs = df[col].dropna().astype(str)\n",
    "        if seqs.empty:\n",
    "            row = {'Region': col}\n",
    "            for aa in aas:\n",
    "                row[f'{aa}_count_total'] = 0\n",
    "                row[f'{aa}_freq_global'] = 0.0\n",
    "                row[f'{aa}_mean_per_seq'] = 0.0\n",
    "                row[f'{aa}_std_per_seq'] = 0.0\n",
    "            rows.append(row)\n",
    "            continue\n",
    "\n",
    "        total_len = sum(len(s) for s in seqs)\n",
    "        totals = Counter()\n",
    "        for s in seqs:\n",
    "            totals.update(s)\n",
    "\n",
    "        per_seq = pd.DataFrame([_counts_per_seq(s, aas=aas) for s in seqs])\n",
    "\n",
    "        row = {'Region': col}\n",
    "        for aa in aas:\n",
    "            aa_total = totals.get(aa, 0)\n",
    "            row[f'{aa}_count_total'] = aa_total\n",
    "            row[f'{aa}_freq_global'] = (aa_total / total_len) if total_len else 0.0\n",
    "            row[f'{aa}_mean_per_seq'] = float(per_seq[aa].mean())\n",
    "            row[f'{aa}_std_per_seq']  = float(per_seq[aa].std(ddof=1)) if len(per_seq) > 1 else 0.0\n",
    "        rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows).set_index('Region')\n",
    "    return out\n",
    "\n",
    "kc_table = kc_region_tables(df_no_outliers)\n",
    "kc_table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ff219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kc_freq_global(kc_table: pd.DataFrame, title=\"K and C  global frequency per region\"):\n",
    "    regions = kc_table.index.tolist()\n",
    "    x = np.arange(len(regions))\n",
    "    width = 0.35\n",
    "\n",
    "    K_freq = kc_table['K_freq_global'].values\n",
    "    C_freq = kc_table['C_freq_global'].values\n",
    "\n",
    "    plt.figure(figsize=(9,5))\n",
    "    palette = plt.get_cmap(\"tab10\")\n",
    "    plt.bar(x - width/2, K_freq, width, label='K (Lysine)', color=palette(0))\n",
    "    plt.bar(x + width/2, C_freq, width, label='C (Cysteine)', color=palette(2))\n",
    "\n",
    "    plt.xticks(x, regions, rotation=0)\n",
    "    plt.ylabel(\"Global frequency (fraction)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_kc_freq_global(kc_table, title=\"K and C frequency  CDRs & FRs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c830a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kc_by_species(df, regions=REGIONS, aas=AAS):\n",
    "    \"\"\"\n",
    "    Compute K and C stats per species and region.\n",
    "    Returns a dict {species: kc_table}.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for sp, sub in df.groupby(\"Species\"):\n",
    "        out[sp] = kc_region_tables(sub, regions=regions, aas=aas)\n",
    "    return out\n",
    "\n",
    "kc_species_tables = kc_by_species(df_cleaned)\n",
    "kc_species_tables[\"Bactrian camel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7383cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kc_species_heatmap(kc_species_tables, value=\"freq_global\", aa=\"K\", title=None):\n",
    "    \"\"\"\n",
    "    Build heatmap for one amino acid (K or C), across species (rows) and regions (cols).\n",
    "    value: one of [\"freq_global\", \"mean_per_seq\", \"count_total\"]\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for sp, tab in kc_species_tables.items():\n",
    "        row = {col: tab.loc[col, f\"{aa}_{value}\"] for col in tab.index}\n",
    "        row[\"Species\"] = sp\n",
    "        frames.append(row)\n",
    "    mat = pd.DataFrame(frames).set_index(\"Species\")\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.heatmap(mat, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "    plt.title(title or f\"{aa}  {value} per region and species\")\n",
    "    plt.xlabel(\"Region\")\n",
    "    plt.ylabel(\"Species\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_kc_species_heatmap(kc_species_tables, value=\"freq_global\", aa=\"K\")\n",
    "plot_kc_species_heatmap(kc_species_tables, value=\"freq_global\", aa=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d6a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
